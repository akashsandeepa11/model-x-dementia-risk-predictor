{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "182HEaBg7WCuydIhET53jLnf3yeU4ZO7Q",
      "authorship_tag": "ABX9TyP2Qm4yHtW4vIw4oL3ZGyRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashsandeepa11/model-x-dementia-risk-predictor/blob/main/randomforest_with_feature_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries for data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Libraries for preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Machine Learning Algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier  # A popular gradient boosting algorithm\n",
        "\n",
        "# Libraries for evaluation\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85zDTMBhWHag",
        "outputId": "122d55aa-cf6c-458e-d137-08aa0cc31678"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPORTANT ---\n",
        "# Load your dataset here.\n",
        "# Change 'nacc_dataset.csv' to the path of your actual file.\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Dementia Prediction Dataset.csv')\n",
        "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Dataset file not found.\")\n",
        "    print(\"Please update the 'pd.read_csv()' line with your file path.\")\n",
        "    # As a placeholder, I'll create an empty DataFrame to allow subsequent cells to run\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "# Display the first 5 rows to understand the data\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo-_yKbXWH3h",
        "outputId": "a49e804e-372f-4fec-c933-68728e012f44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully. Shape: (195196, 1024)\n",
            "       NACCID  NACCADC PACKET  FORMVER  VISITMO  VISITDAY  VISITYR  NACCVNUM  \\\n",
            "0  NACC002909      186      I      3.0       12        28     2022         1   \n",
            "1  NACC002909      186      F      3.0        1        23     2024         2   \n",
            "2  NACC003487      186      I      3.0       11        15     2023         1   \n",
            "3  NACC004352      186      I      3.0       10         5     2021         1   \n",
            "4  NACC004687      186      I      3.0       11        14     2022         1   \n",
            "\n",
            "   NACCAVST  NACCNVST  ...  NPATGAM1  NPATGAM2  NPATGAM3  NPATGAM4  NPATGAM5  \\\n",
            "0         2         2  ...        -4        -4        -4        -4        -4   \n",
            "1         2         2  ...        -4        -4        -4        -4        -4   \n",
            "2         1         1  ...        -4        -4        -4        -4        -4   \n",
            "3         1         1  ...        -4        -4        -4        -4        -4   \n",
            "4         1         1  ...        -4        -4        -4        -4        -4   \n",
            "\n",
            "   NPATGFRN  NPATGFR1  NPATGFR2  NPATGFR3  NPATGFR4  \n",
            "0        -4        -4        -4        -4        -4  \n",
            "1        -4        -4        -4        -4        -4  \n",
            "2        -4        -4        -4        -4        -4  \n",
            "3        -4        -4        -4        -4        -4  \n",
            "4        -4        -4        -4        -4        -4  \n",
            "\n",
            "[5 rows x 1024 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    # 1. DEFINE YOUR TARGET VARIABLE\n",
        "    TARGET_VARIABLE = 'DEMENTED'\n",
        "\n",
        "    # 2. DEFINE THE MASTER LIST of all possible non-medical features\n",
        "    ALL_POSSIBLE_FEATURES = [\n",
        "        # Form A1: Subject Demographics\n",
        "        'NACCAGE', 'SEX', 'EDUC', 'MARISTAT', 'NACCLIVS', 'RESIDENC', 'HANDED',\n",
        "        'HISPANIC', 'RACE', 'RACESEC', 'RACETER', 'PRIMLANG', 'INDEPEND',\n",
        "\n",
        "        # Form A2: Co-participant Demographics\n",
        "        'INRELTO', 'INLIVWTH', 'INVISITS', 'INCALLS',\n",
        "\n",
        "        # Form A3: Subject Family History\n",
        "        'NACCFAM', 'NACCMOM', 'NACCDAD',\n",
        "\n",
        "        # Form A4: Subject Medications\n",
        "        'ANYMEDS',\n",
        "\n",
        "        # Form A5: Subject Health History\n",
        "        'TOBAC30', 'TOBAC100', 'SMOKYRS', 'PACKSPER', 'QUITSMOK', 'ALCOCCAS',\n",
        "        'ALCFREQ', 'ALCOHOL', 'ABUSOTHR', 'CVHATT', 'CVAFIB', 'CVANGIO',\n",
        "        'CVBYPASS', 'CVPACDEF', 'CVPACE', 'CVCHF', 'CVANGINA', 'CVHVALVE',\n",
        "        'CVOTHR', 'HYPERTEN', 'HYPERCHO', 'CBSTROKE', 'NACCSTYR', 'CBTIA',\n",
        "        'NACCTIYR', 'PD', 'PDYR', 'SEIZURES', 'NACCTBI', 'DIABETES', 'DIABTYPE',\n",
        "        'B12DEF', 'THYROID', 'ARTHRIT', 'APNEA', 'RBD', 'INSOMN', 'OTHSLEEP',\n",
        "        'PTSD', 'BIPOLAR', 'SCHIZ', 'DEP2YRS', 'DEPOTHR', 'ANXIETY', 'OCD',\n",
        "        'INCONTU', 'INCONTF',\n",
        "\n",
        "        # Form B1: Physical\n",
        "        'HEIGHT', 'WEIGHT', 'NACCBMI', 'VISION', 'VISCORR', 'VISWCORR',\n",
        "        'HEARING', 'HEARAID', 'HEARWAID',\n",
        "\n",
        "        # Form B9: Self-Reported Decline\n",
        "        'DECSUB', 'DECIN',\n",
        "\n",
        "        # Form B7: Functional Activities\n",
        "        'BILLS', 'TAXES', 'SHOPPING', 'GAMES', 'STOVE', 'MEALPREP',\n",
        "        'EVENTS', 'PAYATTN', 'REMDATES', 'TRAVEL',\n",
        "\n",
        "        # Milestones Form\n",
        "        'NACCNURP',\n",
        "\n",
        "        # Form CLS: Linguistic History (These are the ones causing the error)\n",
        "        'APREFLAN', 'AYRSPAN', 'AYRENGL', 'APCSPAN', 'APCENGL',\n",
        "        'NACCSPNL', 'NACCENGL'\n",
        "    ]\n",
        "\n",
        "    # 3. CRITICAL STEP: Filter the list to only include features in your CSV\n",
        "    NON_MEDICAL_FEATURES = [col for col in ALL_POSSIBLE_FEATURES if col in df.columns]\n",
        "\n",
        "    # Find and report any missing features (for your information)\n",
        "    missing_from_csv = [col for col in ALL_POSSIBLE_FEATURES if col not in df.columns]\n",
        "\n",
        "    print(f\"Target variable set to: {TARGET_VARIABLE}\")\n",
        "    print(f\"Found {len(NON_MEDICAL_FEATURES)} available non-medical features in your file.\")\n",
        "\n",
        "    if missing_from_csv:\n",
        "        print(\"\\nNote: The following features from the data dictionary were not found in your file and will be skipped:\")\n",
        "        print(missing_from_csv)\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm0HczxUWSOU",
        "outputId": "1029a668-6945-432e-b1fb-c8e43092ae13"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target variable set to: DEMENTED\n",
            "Found 90 available non-medical features in your file.\n",
            "\n",
            "Note: The following features from the data dictionary were not found in your file and will be skipped:\n",
            "['APREFLAN', 'AYRSPAN', 'AYRENGL', 'APCSPAN', 'APCENGL', 'NACCSPNL', 'NACCENGL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(X_df):\n",
        "    \"\"\"\n",
        "    Takes the feature DataFrame (X) and adds new engineered features.\n",
        "    \"\"\"\n",
        "    # Make a copy to avoid changing the original data\n",
        "    X_engineered = X_df.copy()\n",
        "\n",
        "    # --- 1. Create Composite Scores (Summing related risks) ---\n",
        "\n",
        "    # List of all functional activity columns (Form B7)\n",
        "    functional_cols = [\n",
        "        'BILLS', 'TAXES', 'SHOPPING', 'GAMES', 'STOVE',\n",
        "        'MEALPREP', 'EVENTS', 'PAYATTN', 'REMDATES', 'TRAVEL'\n",
        "    ]\n",
        "    # Fill NaNs with 0 (assume 'no difficulty') before summing\n",
        "    X_engineered['FUNC_DIFFICULTY_SCORE'] = X_engineered[functional_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "    # List of self-reported cardiovascular conditions (Form A5)\n",
        "    cvd_cols = [\n",
        "        'CVHATT', 'CVAFIB', 'CVANGIO', 'CVBYPASS', 'CVPACDEF', 'CVPACE',\n",
        "        'CVCHF', 'CVANGINA', 'CVHVALVE', 'HYPERTEN', 'HYPERCHO', 'CBSTROKE', 'CBTIA'\n",
        "    ]\n",
        "    # Fill NaNs with 0 (assume 'absent') before summing to get a risk count\n",
        "    X_engineered['CVD_RISK_COUNT'] = X_engineered[cvd_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "    # List of self-reported psychiatric conditions (Form A5)\n",
        "    psych_cols = [\n",
        "        'PTSD', 'BIPOLAR', 'SCHIZ', 'DEP2YRS', 'DEPOTHR', 'ANXIETY', 'OCD'\n",
        "    ]\n",
        "    X_engineered['PSYCH_RISK_COUNT'] = X_engineered[psych_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "\n",
        "    # --- 2. Bin Numerical Features (Turning numbers into categories) ---\n",
        "\n",
        "    # Bin BMI into standard categories\n",
        "    bmi_bins = [0, 18.5, 24.9, 29.9, 100]\n",
        "    bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese']\n",
        "    X_engineered['BMI_CATEGORY'] = pd.cut(X_engineered['NACCBMI'], bins=bmi_bins, labels=bmi_labels, right=False)\n",
        "\n",
        "    # Bin Age into groups\n",
        "    age_bins = [0, 65, 75, 85, 120]\n",
        "    age_labels = ['65_or_less', '66-75', '76-85', '86_and_up']\n",
        "    X_engineered['AGE_GROUP'] = pd.cut(X_engineered['NACCAGE'], bins=age_bins, labels=age_labels, right=False)\n",
        "\n",
        "\n",
        "    # --- 3. Create Interaction Features ---\n",
        "\n",
        "    # Check interaction between age and education\n",
        "    # (The pipeline will handle imputing NaNs in the original NACCAGE/EDUC columns)\n",
        "    X_engineered['AGE_X_EDUC'] = X_engineered['NACCAGE'] * X_engineered['EDUC']\n",
        "\n",
        "    print(\"Feature engineering complete.\")\n",
        "    print(f\"Original features: {len(X_df.columns)}, New features: {len(X_engineered.columns)}\")\n",
        "\n",
        "    return X_engineered"
      ],
      "metadata": {
        "id": "ErXBCKpnWtQH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    # 1. --- Select X and y ---\n",
        "    df_clean = df.dropna(subset=[TARGET_VARIABLE])\n",
        "\n",
        "    # NON_MEDICAL_FEATURES should be available from running Cell 3\n",
        "    X = df_clean[NON_MEDICAL_FEATURES]\n",
        "    y = df_clean[TARGET_VARIABLE].astype(int)\n",
        "\n",
        "    # 2. --- Data Cleaning ---\n",
        "    MISSING_CODES = [\n",
        "        -4, 8, 9, 88, 99, 888, 999, 8888, 9999,\n",
        "        95, 96, 97, 98, 995, 996, 997, 998\n",
        "    ]\n",
        "    X = X.replace(MISSING_CODES, np.nan)\n",
        "    print(f\"Missing values (as np.nan) identified: {X.isna().sum().sum()}\")\n",
        "\n",
        "    # 3. --- Apply Feature Engineering ---\n",
        "    # This calls the function from Cell 4\n",
        "    X_engineered = engineer_features(X)\n",
        "\n",
        "    # 4. --- Define Preprocessing Pipelines (with new features) ---\n",
        "\n",
        "    # --- FIX IS HERE ---\n",
        "    # This list needs to be defined within this cell\n",
        "    ALL_POSSIBLE_NUMERIC_FEATURES = [\n",
        "        'NACCAGE', 'EDUC', 'SMOKYRS', 'PACKSPER', 'QUITSMOK', 'NACCSTYR',\n",
        "        'NACCTIYR', 'PDYR', 'HEIGHT', 'WEIGHT', 'NACCBMI', 'AYRSPAN',\n",
        "        'AYRENGL', 'APCSPAN', 'APCENGL'\n",
        "    ]\n",
        "    # --- END FIX ---\n",
        "\n",
        "    # Get the original feature lists\n",
        "    NUMERIC_FEATURES_ORIGINAL = [col for col in NON_MEDICAL_FEATURES if col in ALL_POSSIBLE_NUMERIC_FEATURES]\n",
        "    CATEGORICAL_FEATURES_ORIGINAL = [col for col in NON_MEDICAL_FEATURES if col not in NUMERIC_FEATURES_ORIGINAL]\n",
        "\n",
        "    # Add our new features to the lists for preprocessing\n",
        "    NUMERIC_FEATURES = NUMERIC_FEATURES_ORIGINAL + [\n",
        "        'FUNC_DIFFICULTY_SCORE', 'CVD_RISK_COUNT', 'PSYCH_RISK_COUNT', 'AGE_X_EDUC'\n",
        "    ]\n",
        "    CATEGORICAL_FEATURES = CATEGORICAL_FEATURES_ORIGINAL + [\n",
        "        'BMI_CATEGORY', 'AGE_GROUP'\n",
        "    ]\n",
        "\n",
        "    # Remove duplicates just in case\n",
        "    NUMERIC_FEATURES = list(set([col for col in NUMERIC_FEATURES if col in X_engineered.columns]))\n",
        "    CATEGORICAL_FEATURES = list(set([col for col in CATEGORICAL_FEATURES if col in X_engineered.columns]))\n",
        "\n",
        "    print(f\"Total numeric features for pipeline: {len(NUMERIC_FEATURES)}\")\n",
        "    print(f\"Total categorical features for pipeline: {len(CATEGORICAL_FEATURES)}\")\n",
        "\n",
        "    # Create the pipelines (same as before)\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Combine pipelines\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, NUMERIC_FEATURES),\n",
        "            ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
        "        ],\n",
        "        remainder='drop' # Drop any original columns we didn't use\n",
        "    )\n",
        "\n",
        "    print(\"Preprocessing pipelines updated with new features.\")\n",
        "\n",
        "    # 5. --- Split the Data ---\n",
        "    # We split the *engineered* data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_engineered, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nData split into training and testing sets.\")\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "id": "HBr7XVd6ky_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69958aa-d91d-49c9-e50b-d582b5a2cc2b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values (as np.nan) identified: 6850211\n",
            "Feature engineering complete.\n",
            "Original features: 90, New features: 96\n",
            "Total numeric features for pipeline: 15\n",
            "Total categorical features for pipeline: 81\n",
            "Preprocessing pipelines updated with new features.\n",
            "\n",
            "Data split into training and testing sets.\n",
            "X_train shape: (156156, 96)\n",
            "X_test shape: (39040, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression (Baseline)\n",
        "\n",
        "if not df.empty:\n",
        "    # Create the full model pipeline\n",
        "    lr_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    # --- Train the model ---\n",
        "    print(\"Training Logistic Regression model on ENGINEERED features...\")\n",
        "    lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    print(\"\\n--- Logistic Regression (Engineered) Evaluation ---\")\n",
        "    y_pred_lr = lr_pipeline.predict(X_test)\n",
        "    y_proba_lr = lr_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
        "    print(f\"AUC Score: {roc_auc_score(y_test, y_proba_lr):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_lr, target_names=[\"Not at risk (0)\", \"At risk (1)\"]))\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2 or 5.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byACfS4dnB9I",
        "outputId": "e7b13e4a-803d-4eac-d708-8c0bbd177fbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression model on ENGINEERED features...\n",
            "\n",
            "--- Logistic Regression (Engineered) Evaluation ---\n",
            "Accuracy: 0.9288\n",
            "AUC Score: 0.9754\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not at risk (0)       0.94      0.96      0.95     27522\n",
            "    At risk (1)       0.90      0.86      0.88     11518\n",
            "\n",
            "       accuracy                           0.93     39040\n",
            "      macro avg       0.92      0.91      0.91     39040\n",
            "   weighted avg       0.93      0.93      0.93     39040\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "\n",
        "if not df.empty:\n",
        "    # Create the full model pipeline\n",
        "    rf_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))\n",
        "    ])\n",
        "\n",
        "    # --- Train the model ---\n",
        "    print(\"Training Random Forest model on ENGINEERED features...\")\n",
        "    rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    print(\"\\n--- Random Forest (Engineered) Evaluation ---\")\n",
        "    y_pred_rf = rf_pipeline.predict(X_test)\n",
        "    y_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "    print(f\"AUC Score: {roc_auc_score(y_test, y_proba_rf):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_rf, target_names=[\"Not at risk (0)\", \"At risk (1)\"]))\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2 or 5.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EAnkz8coaAE",
        "outputId": "0d42d45a-e1de-4b9f-89d6-f1c9dac26907"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Random Forest model on ENGINEERED features...\n",
            "\n",
            "--- Random Forest (Engineered) Evaluation ---\n",
            "Accuracy: 0.9342\n",
            "AUC Score: 0.9788\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not at risk (0)       0.95      0.96      0.95     27522\n",
            "    At risk (1)       0.90      0.88      0.89     11518\n",
            "\n",
            "       accuracy                           0.93     39040\n",
            "      macro avg       0.92      0.92      0.92     39040\n",
            "   weighted avg       0.93      0.93      0.93     39040\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifier\n",
        "\n",
        "if not df.empty:\n",
        "    # Create the full model pipeline\n",
        "    xgb_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
        "    ])\n",
        "\n",
        "    # --- Train the model ---\n",
        "    print(\"Training XGBoost model on ENGINEERED features...\")\n",
        "    xgb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    print(\"\\n--- XGBoost (Engineered) Evaluation ---\")\n",
        "    y_pred_xgb = xgb_pipeline.predict(X_test)\n",
        "    y_proba_xgb = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "    print(f\"AUC Score: {roc_auc_score(y_test, y_proba_xgb):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_xgb, target_names=[\"Not at risk (0)\", \"At risk (1)\"]))\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2 or 5.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0BNhrjaorrW",
        "outputId": "6139ae8e-5058-4778-d63a-778f03f06240"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGBoost model on ENGINEERED features...\n",
            "\n",
            "--- XGBoost (Engineered) Evaluation ---\n",
            "Accuracy: 0.9337\n",
            "AUC Score: 0.9792\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not at risk (0)       0.95      0.96      0.95     27522\n",
            "    At risk (1)       0.89      0.88      0.89     11518\n",
            "\n",
            "       accuracy                           0.93     39040\n",
            "      macro avg       0.92      0.92      0.92     39040\n",
            "   weighted avg       0.93      0.93      0.93     39040\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    print(\"--- Model Performance Summary (with Engineered Features) ---\")\n",
        "\n",
        "    results_engineered = {\n",
        "        'Model': ['Logistic Regression (Eng)', 'Random Forest (Eng)', 'XGBoost (Eng)'],\n",
        "        'Accuracy': [\n",
        "            accuracy_score(y_test, y_pred_lr),\n",
        "            accuracy_score(y_test, y_pred_rf),\n",
        "            accuracy_score(y_test, y_pred_xgb)\n",
        "        ],\n",
        "        'AUC Score': [\n",
        "            roc_auc_score(y_test, y_proba_lr),\n",
        "            roc_auc_score(y_test, y_proba_rf),\n",
        "            roc_auc_score(y_test, y_proba_xgb)\n",
        "        ],\n",
        "        'F1-Score (At risk)': [\n",
        "            f1_score(y_test, y_pred_lr, pos_label=1),\n",
        "            f1_score(y_test, y_pred_rf, pos_label=1),\n",
        "            f1_score(y_test, y_pred_xgb, pos_label=1)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results_engineered_df = pd.DataFrame(results_engineered)\n",
        "    print(results_engineered_df.to_markdown(index=False, floatfmt=\".4f\"))\n",
        "\n",
        "    print(\"\\n\\nCompare this table to your first run. Did the new features help?\")\n",
        "    print(\"Your next steps are still to tune the best model (Cell 10) and check explainability (Cell 12).\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please run the model cells first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_QaNivepKo7",
        "outputId": "8079d240-a241-4719-e70a-eac7ab331b36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Performance Summary (with Engineered Features) ---\n",
            "| Model                     |   Accuracy |   AUC Score |   F1-Score (At risk) |\n",
            "|:--------------------------|-----------:|------------:|---------------------:|\n",
            "| Logistic Regression (Eng) |     0.9288 |      0.9754 |               0.8768 |\n",
            "| Random Forest (Eng)       |     0.9342 |      0.9788 |               0.8874 |\n",
            "| XGBoost (Eng)             |     0.9337 |      0.9792 |               0.8868 |\n",
            "\n",
            "\n",
            "Compare this table to your first run. Did the new features help?\n",
            "Your next steps are still to tune the best model (Cell 10) and check explainability (Cell 12).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"Starting Hyperparameter Tuning for Random Forest...\")\n",
        "\n",
        "    # 1. Re-create the pipeline\n",
        "    # We use the 'rf_pipeline' variable name from Cell 7\n",
        "    rf_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(random_state=42))\n",
        "    ])\n",
        "\n",
        "    # 2. Define the 'parameter grid' to search\n",
        "    # These are common parameters to tune for a Random Forest\n",
        "    param_grid_rf = {\n",
        "        'classifier__n_estimators': [100, 200],         # Number of trees\n",
        "        'classifier__max_depth': [5, 10, None],          # Max depth of each tree (None = no limit)\n",
        "        'classifier__min_samples_leaf': [1, 2]           # Min samples required at a leaf node\n",
        "    }\n",
        "\n",
        "    # 3. Set up the Grid Search\n",
        "    # We'll name this 'grid_search_rf' to avoid overwriting the XGBoost one\n",
        "    grid_search_rf = GridSearchCV(\n",
        "        estimator=rf_pipeline,\n",
        "        param_grid=param_grid_rf,\n",
        "        scoring='roc_auc',  # Optimize for the best AUC score\n",
        "        cv=3,               # 3-fold cross-validation\n",
        "        verbose=1,          # Print updates\n",
        "        n_jobs=-1           # Use all available CPU cores\n",
        "    )\n",
        "\n",
        "    # 4. Run the Grid Search (This will take a few minutes)\n",
        "    grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Show the results\n",
        "    print(\"\\n--- Tuning Complete (Random Forest) ---\")\n",
        "    print(f\"Best AUC Score found: {grid_search_rf.best_score_:.4f}\")\n",
        "    print(\"Best parameters found:\")\n",
        "    print(grid_search_rf.best_params_)\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please run Cell 5 first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RC6zrsBpb4F",
        "outputId": "6432c5bb-497c-4549-a03c-68948f142ba7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Hyperparameter Tuning for Random Forest...\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "\n",
            "--- Tuning Complete (Random Forest) ---\n",
            "Best AUC Score found: 0.9775\n",
            "Best parameters found:\n",
            "{'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__n_estimators': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty and 'grid_search_rf' in locals():\n",
        "    print(\"--- Evaluating the BEST Tuned Random Forest on Test Data ---\")\n",
        "\n",
        "    # Get the best model found by the grid search\n",
        "    best_rf_model = grid_search_rf.best_estimator_\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    y_pred_best_rf = best_rf_model.predict(X_test)\n",
        "    y_proba_best_rf = best_rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_best_rf):.4f}\")\n",
        "    print(f\"AUC Score: {roc_auc_score(y_test, y_proba_best_rf):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_best_rf, target_names=[\"Not at risk (0)\", \"At risk (1)\"]))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_best_rf))\n",
        "\n",
        "else:\n",
        "    print(\"Please run Cell 10 to complete the Random Forest grid search first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4ubgaCLqo8_",
        "outputId": "8c252aa3-e92b-4c7a-c0a7-febdf54b99db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Evaluating the BEST Tuned Random Forest on Test Data ---\n",
            "Accuracy: 0.9336\n",
            "AUC Score: 0.9792\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not at risk (0)       0.95      0.96      0.95     27522\n",
            "    At risk (1)       0.89      0.88      0.89     11518\n",
            "\n",
            "       accuracy                           0.93     39040\n",
            "      macro avg       0.92      0.92      0.92     39040\n",
            "   weighted avg       0.93      0.93      0.93     39040\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26323  1199]\n",
            " [ 1394 10124]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5b098q-vyoAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}