{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1O6pwbv2B5E0xBHFlF4xuJGTGflb9aNKB",
      "authorship_tag": "ABX9TyNG6pMiLtavdsio5OltkL7v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashsandeepa11/model-x-dementia-risk-predictor/blob/main/Model_x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries for data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Libraries for preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Machine Learning Algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier  # A popular gradient boosting algorithm\n",
        "\n",
        "# Libraries for evaluation\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85zDTMBhWHag",
        "outputId": "a0abd8c1-907a-4511-c5e7-006e3e6660ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPORTANT ---\n",
        "# Load your dataset here.\n",
        "# Change 'nacc_dataset.csv' to the path of your actual file.\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Dementia Prediction Dataset.csv')\n",
        "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Dataset file not found.\")\n",
        "    print(\"Please update the 'pd.read_csv()' line with your file path.\")\n",
        "    # As a placeholder, I'll create an empty DataFrame to allow subsequent cells to run\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "# Display the first 5 rows to understand the data\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo-_yKbXWH3h",
        "outputId": "0d65f92e-8ef1-4f01-ac80-285b5cbcfab6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully. Shape: (195196, 1024)\n",
            "       NACCID  NACCADC PACKET  FORMVER  VISITMO  VISITDAY  VISITYR  NACCVNUM  \\\n",
            "0  NACC002909      186      I      3.0       12        28     2022         1   \n",
            "1  NACC002909      186      F      3.0        1        23     2024         2   \n",
            "2  NACC003487      186      I      3.0       11        15     2023         1   \n",
            "3  NACC004352      186      I      3.0       10         5     2021         1   \n",
            "4  NACC004687      186      I      3.0       11        14     2022         1   \n",
            "\n",
            "   NACCAVST  NACCNVST  ...  NPATGAM1  NPATGAM2  NPATGAM3  NPATGAM4  NPATGAM5  \\\n",
            "0         2         2  ...        -4        -4        -4        -4        -4   \n",
            "1         2         2  ...        -4        -4        -4        -4        -4   \n",
            "2         1         1  ...        -4        -4        -4        -4        -4   \n",
            "3         1         1  ...        -4        -4        -4        -4        -4   \n",
            "4         1         1  ...        -4        -4        -4        -4        -4   \n",
            "\n",
            "   NPATGFRN  NPATGFR1  NPATGFR2  NPATGFR3  NPATGFR4  \n",
            "0        -4        -4        -4        -4        -4  \n",
            "1        -4        -4        -4        -4        -4  \n",
            "2        -4        -4        -4        -4        -4  \n",
            "3        -4        -4        -4        -4        -4  \n",
            "4        -4        -4        -4        -4        -4  \n",
            "\n",
            "[5 rows x 1024 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    # 1. DEFINE YOUR TARGET VARIABLE\n",
        "    TARGET_VARIABLE = 'DEMENTED'\n",
        "\n",
        "    # 2. DEFINE THE MASTER LIST of all possible non-medical features\n",
        "    ALL_POSSIBLE_FEATURES = [\n",
        "        # Form A1: Subject Demographics\n",
        "        'NACCAGE', 'SEX', 'EDUC', 'MARISTAT', 'NACCLIVS', 'RESIDENC', 'HANDED',\n",
        "        'HISPANIC', 'RACE', 'RACESEC', 'RACETER', 'PRIMLANG', 'INDEPEND',\n",
        "\n",
        "        # Form A2: Co-participant Demographics\n",
        "        'INRELTO', 'INLIVWTH', 'INVISITS', 'INCALLS',\n",
        "\n",
        "        # Form A3: Subject Family History\n",
        "        'NACCFAM', 'NACCMOM', 'NACCDAD',\n",
        "\n",
        "        # Form A4: Subject Medications\n",
        "        'ANYMEDS',\n",
        "\n",
        "        # Form A5: Subject Health History\n",
        "        'TOBAC30', 'TOBAC100', 'SMOKYRS', 'PACKSPER', 'QUITSMOK', 'ALCOCCAS',\n",
        "        'ALCFREQ', 'ALCOHOL', 'ABUSOTHR', 'CVHATT', 'CVAFIB', 'CVANGIO',\n",
        "        'CVBYPASS', 'CVPACDEF', 'CVPACE', 'CVCHF', 'CVANGINA', 'CVHVALVE',\n",
        "        'CVOTHR', 'HYPERTEN', 'HYPERCHO', 'CBSTROKE', 'NACCSTYR', 'CBTIA',\n",
        "        'NACCTIYR', 'PD', 'PDYR', 'SEIZURES', 'NACCTBI', 'DIABETES', 'DIABTYPE',\n",
        "        'B12DEF', 'THYROID', 'ARTHRIT', 'APNEA', 'RBD', 'INSOMN', 'OTHSLEEP',\n",
        "        'PTSD', 'BIPOLAR', 'SCHIZ', 'DEP2YRS', 'DEPOTHR', 'ANXIETY', 'OCD',\n",
        "        'INCONTU', 'INCONTF',\n",
        "\n",
        "        # Form B1: Physical\n",
        "        'HEIGHT', 'WEIGHT', 'NACCBMI', 'VISION', 'VISCORR', 'VISWCORR',\n",
        "        'HEARING', 'HEARAID', 'HEARWAID',\n",
        "\n",
        "        # Form B9: Self-Reported Decline\n",
        "        'DECSUB', 'DECIN',\n",
        "\n",
        "        # Form B7: Functional Activities\n",
        "        'BILLS', 'TAXES', 'SHOPPING', 'GAMES', 'STOVE', 'MEALPREP',\n",
        "        'EVENTS', 'PAYATTN', 'REMDATES', 'TRAVEL',\n",
        "\n",
        "        # Milestones Form\n",
        "        'NACCNURP',\n",
        "\n",
        "        # Form CLS: Linguistic History (These are the ones causing the error)\n",
        "        'APREFLAN', 'AYRSPAN', 'AYRENGL', 'APCSPAN', 'APCENGL',\n",
        "        'NACCSPNL', 'NACCENGL'\n",
        "    ]\n",
        "\n",
        "    # 3. CRITICAL STEP: Filter the list to only include features in your CSV\n",
        "    NON_MEDICAL_FEATURES = [col for col in ALL_POSSIBLE_FEATURES if col in df.columns]\n",
        "\n",
        "    # Find and report any missing features (for your information)\n",
        "    missing_from_csv = [col for col in ALL_POSSIBLE_FEATURES if col not in df.columns]\n",
        "\n",
        "    print(f\"Target variable set to: {TARGET_VARIABLE}\")\n",
        "    print(f\"Found {len(NON_MEDICAL_FEATURES)} available non-medical features in your file.\")\n",
        "\n",
        "    if missing_from_csv:\n",
        "        print(\"\\nNote: The following features from the data dictionary were not found in your file and will be skipped:\")\n",
        "        print(missing_from_csv)\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm0HczxUWSOU",
        "outputId": "a9708907-146c-4074-9b07-7e63f953cd14"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target variable set to: DEMENTED\n",
            "Found 90 available non-medical features in your file.\n",
            "\n",
            "Note: The following features from the data dictionary were not found in your file and will be skipped:\n",
            "['APREFLAN', 'AYRSPAN', 'AYRENGL', 'APCSPAN', 'APCENGL', 'NACCSPNL', 'NACCENGL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    # 1. --- Select X and y ---\n",
        "\n",
        "    # Drop rows where the target variable is missing\n",
        "    df_clean = df.dropna(subset=[TARGET_VARIABLE])\n",
        "\n",
        "    # Use our new *filtered* list of features\n",
        "    X = df_clean[NON_MEDICAL_FEATURES]\n",
        "    y = df_clean[TARGET_VARIABLE].astype(int)\n",
        "\n",
        "    # 2. --- Data Cleaning (Best Practice) ---\n",
        "\n",
        "    # Define all codes from the Data Dictionary that mean \"Unknown\" or \"Not Assessed\"\n",
        "    MISSING_CODES = [\n",
        "        -4, 8, 9, 88, 99, 888, 999, 8888, 9999,\n",
        "        95, 96, 97, 98, 995, 996, 997, 998\n",
        "    ]\n",
        "\n",
        "    print(f\"Original missing values in X: {X.isna().sum().sum()}\")\n",
        "\n",
        "    # Replace all special missing codes with np.nan\n",
        "    X = X.replace(MISSING_CODES, np.nan)\n",
        "\n",
        "    print(f\"Total missing values after cleaning (now np.nan): {X.isna().sum().sum()}\")\n",
        "\n",
        "    # 3. --- Define Preprocessing Pipelines (Now fully dynamic) ---\n",
        "\n",
        "    # Define the *master list* of known numeric features\n",
        "    ALL_POSSIBLE_NUMERIC_FEATURES = [\n",
        "        'NACCAGE', 'EDUC', 'SMOKYRS', 'PACKSPER', 'QUITSMOK', 'NACCSTYR',\n",
        "        'NACCTIYR', 'PDYR', 'HEIGHT', 'WEIGHT', 'NACCBMI', 'AYRSPAN',\n",
        "        'AYRENGL', 'APCSPAN', 'APCENGL'\n",
        "    ]\n",
        "\n",
        "    # Filter NUMERIC_FEATURES to only those in our available NON_MEDICAL_FEATURES\n",
        "    NUMERIC_FEATURES = [col for col in NON_MEDICAL_FEATURES if col in ALL_POSSIBLE_NUMERIC_FEATURES]\n",
        "\n",
        "    # CATEGORICAL_FEATURES is everything else that's left\n",
        "    CATEGORICAL_FEATURES = [col for col in NON_MEDICAL_FEATURES if col not in NUMERIC_FEATURES]\n",
        "\n",
        "    print(f\"\\nIdentified {len(NUMERIC_FEATURES)} numeric features.\")\n",
        "    print(f\"Identified {len(CATEGORICAL_FEATURES)} categorical features.\")\n",
        "\n",
        "    # Pipeline for numerical data:\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Pipeline for categorical data:\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # Combine both pipelines into a single preprocessor\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, NUMERIC_FEATURES),\n",
        "            ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    print(\"Preprocessing pipelines defined successfully.\")\n",
        "\n",
        "    # 4. --- Split the Data ---\n",
        "\n",
        "    # Split the cleaned data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,    # 20% for testing\n",
        "        random_state=42,  # For reproducible results\n",
        "        stratify=y        # Critical for classification\n",
        "    )\n",
        "\n",
        "    print(f\"\\nData split into training and testing sets.\")\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErXBCKpnWtQH",
        "outputId": "6cd01fee-c190-4572-b54f-b0401cf46458"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original missing values in X: 970\n",
            "Total missing values after cleaning (now np.nan): 6850211\n",
            "\n",
            "Identified 11 numeric features.\n",
            "Identified 79 categorical features.\n",
            "Preprocessing pipelines defined successfully.\n",
            "\n",
            "Data split into training and testing sets.\n",
            "X_train shape: (156156, 90)\n",
            "X_test shape: (39040, 90)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression (Baseline)\n",
        "\n",
        "if not df.empty:\n",
        "    # Create the full model pipeline\n",
        "    lr_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    # --- Train the model ---\n",
        "    print(\"Training Logistic Regression model...\")\n",
        "    lr_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    print(\"\\n--- Logistic Regression Evaluation ---\")\n",
        "    y_pred_lr = lr_pipeline.predict(X_test)\n",
        "    y_proba_lr = lr_pipeline.predict_proba(X_test)[:, 1] # Probability of \"1\" (Dementia)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
        "    print(f\"AUC Score: {roc_auc_score(y_test, y_proba_lr):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_lr, target_names=[\"Not at risk (0)\", \"At risk (1)\"]))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_lr))\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtO1RFm9WyeI",
        "outputId": "266752f0-3fb2-4832-d50b-423d29143a70"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression model...\n",
            "\n",
            "--- Logistic Regression Evaluation ---\n",
            "Accuracy: 0.9287\n",
            "AUC Score: 0.9752\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not at risk (0)       0.94      0.96      0.95     27522\n",
            "    At risk (1)       0.90      0.86      0.88     11518\n",
            "\n",
            "       accuracy                           0.93     39040\n",
            "      macro avg       0.92      0.91      0.91     39040\n",
            "   weighted avg       0.93      0.93      0.93     39040\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26389  1133]\n",
            " [ 1651  9867]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "\n",
        "if not df.empty:\n",
        "    # Create the full model pipeline\n",
        "    rf_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))\n",
        "    ])\n",
        "\n",
        "    # --- Train the model ---\n",
        "    print(\"Training Random Forest model...\")\n",
        "    rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    print(\"\\n--- Random Forest Evaluation ---\")\n",
        "    y_pred_rf = rf_pipeline.predict(X_test)\n",
        "    y_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1] # Probability of \"1\" (Dementia)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "    print(f\"AUC Score: {roc_auc_score(y_test, y_proba_rf):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_rf, target_names=[\"Not at risk (0)\", \"At risk (1)\"]))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddhq53VMZ31_",
        "outputId": "1872bf7f-f8f1-4591-e429-5e77f0c2f809"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Random Forest model...\n",
            "\n",
            "--- Random Forest Evaluation ---\n",
            "Accuracy: 0.9332\n",
            "AUC Score: 0.9783\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not at risk (0)       0.95      0.96      0.95     27522\n",
            "    At risk (1)       0.89      0.88      0.89     11518\n",
            "\n",
            "       accuracy                           0.93     39040\n",
            "      macro avg       0.92      0.92      0.92     39040\n",
            "   weighted avg       0.93      0.93      0.93     39040\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26329  1193]\n",
            " [ 1415 10103]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Classifier (Gradient Boosting)\n",
        "\n",
        "if not df.empty:\n",
        "    # Create the full model pipeline\n",
        "    xgb_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
        "    ])\n",
        "\n",
        "    # --- Train the model ---\n",
        "    print(\"Training XGBoost model...\")\n",
        "    xgb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # --- Evaluate the model ---\n",
        "    print(\"\\n--- XGBoost Evaluation ---\")\n",
        "    y_pred_xgb = xgb_pipeline.predict(X_test)\n",
        "    y_proba_xgb = xgb_pipeline.predict_proba(X_test)[:, 1] # Probability of \"1\" (Dementia)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "    print(f\"AUC Score: {roc_auc_score(y_test, y_proba_xgb):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_xgb, target_names=[\"Not at risk (0)\", \"At risk (1)\"]))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_xgb))\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s65iYgnnahzr",
        "outputId": "77753322-9aed-4a0e-840a-ad5696a9daa7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGBoost model...\n",
            "\n",
            "--- XGBoost Evaluation ---\n",
            "Accuracy: 0.9345\n",
            "AUC Score: 0.9790\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Not at risk (0)       0.95      0.96      0.95     27522\n",
            "    At risk (1)       0.90      0.88      0.89     11518\n",
            "\n",
            "       accuracy                           0.93     39040\n",
            "      macro avg       0.92      0.92      0.92     39040\n",
            "   weighted avg       0.93      0.93      0.93     39040\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26355  1167]\n",
            " [ 1390 10128]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not df.empty:\n",
        "    print(\"--- Model Performance Summary ---\")\n",
        "\n",
        "    # Create a simple DataFrame for comparison\n",
        "    results = {\n",
        "        'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
        "        'Accuracy': [\n",
        "            accuracy_score(y_test, y_pred_lr),\n",
        "            accuracy_score(y_test, y_pred_rf),\n",
        "            accuracy_score(y_test, y_pred_xgb)\n",
        "        ],\n",
        "        'AUC Score': [\n",
        "            roc_auc_score(y_test, y_proba_lr),\n",
        "            roc_auc_score(y_test, y_proba_rf),\n",
        "            roc_auc_score(y_test, y_proba_xgb)\n",
        "        ],\n",
        "        'F1-Score (At risk)': [\n",
        "            f1_score(y_test, y_pred_lr, pos_label=1),\n",
        "            f1_score(y_test, y_pred_rf, pos_label=1),\n",
        "            f1_score(y_test, y_pred_xgb, pos_label=1)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(results_df.to_markdown(index=False, floatfmt=\".4f\"))\n",
        "\n",
        "    print(\"\\n\\n--- Next Steps (Hackathon Requirements) ---\")\n",
        "    print(\"1.  **Hyperparameter Tuning:** Use GridSearchCV or RandomizedSearchCV on your best-performing model (e.g., Random Forest or XGBoost) to find better settings[cite: 1421].\")\n",
        "    print(\"2.  **Feature Engineering:** Try creating new features. For example, 'Age x Education' or 'BMI_Category' from NACCBMI[cite: 1421].\")\n",
        "    print(\"3.  **Explainability:** For your report, get the 'feature_importances_' from the trained Random Forest or XGBoost model to see which non-medical factors were most predictive[cite: 1421].\")\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame is empty. Please load data in Cell 2.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWZtbCdrbq1_",
        "outputId": "73f1151d-b3f7-410f-b835-69e7ec803170"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Performance Summary ---\n",
            "| Model               |   Accuracy |   AUC Score |   F1-Score (At risk) |\n",
            "|:--------------------|-----------:|------------:|---------------------:|\n",
            "| Logistic Regression |     0.9287 |      0.9752 |               0.8764 |\n",
            "| Random Forest       |     0.9332 |      0.9783 |               0.8857 |\n",
            "| XGBoost             |     0.9345 |      0.9790 |               0.8879 |\n",
            "\n",
            "\n",
            "--- Next Steps (Hackathon Requirements) ---\n",
            "1.  **Hyperparameter Tuning:** Use GridSearchCV or RandomizedSearchCV on your best-performing model (e.g., Random Forest or XGBoost) to find better settings[cite: 1421].\n",
            "2.  **Feature Engineering:** Try creating new features. For example, 'Age x Education' or 'BMI_Category' from NACCBMI[cite: 1421].\n",
            "3.  **Explainability:** For your report, get the 'feature_importances_' from the trained Random Forest or XGBoost model to see which non-medical factors were most predictive[cite: 1421].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vu7nZ5zPcWKF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}